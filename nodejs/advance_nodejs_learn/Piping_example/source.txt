# [ELK stack](https://aws.amazon.com/what-is/elk-stack/#:~:text=The%20ELK%20stack%20is%20an,Elasticsearch%2C%20Logstash%2C%20and%20Kibana.) version 8.13:

- This is a multi-node elasticsearch setup for ELK stack for collecting application logs
- This has SSL enabled, it is close to ready for production
- [Elasticsearch](https://aws.amazon.com/what-is/elasticsearch/) is used to store huge amounts of indexed data. It is no-sql data storage.
- [Kibana](https://www.elastic.co/kibana) is used for visualizing the data. It is very powerful.
- [Logstash](https://www.elastic.co/logstash) is used for collecting the data and sending the data to elastic search
- We have setup [winston](https://www.npmjs.com/package/winston) logger and [@elastic/ecs-winston-format](https://www.npmjs.com/package/@elastic/ecs-winston-format) to send logs from the NodeJS to logstash
- There are 3 Elastic nodes `setup`, `es01`,and `es02`. The `setup` service is primarly used for setting up the entire stack. It will automatically exit after the setup is done.
- `es01` is the master node and `es02` is the seed node. The seed node is primary used for redundancy, in case of data loss.

## Sources:

1. [Youtube video](https://www.youtube.com/watch?v=jXU_1GADENQ)
1. [Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/8.13/docker.html#docker-compose-file) on how to setup multiple nodes


# Steps to run:

1. Take a copy of `devOps/ELK/.env.sample` and rename it to `devOps/ELK/.env`. This will have the passwords for elastic, kibana, cluster name and memory limits for ELK stack. Increase max virtual memory:

```bash
sudo sysctl -w vm.max_map_count=262144
```

2. Make sure that the services from which you want to receive the log from has the logging section in the docker compose file as show below:

```yml
api:
    build:
      context: .
      dockerfile: api.Dockerfile
      args:
        NODE_ENV: development
    container_name: CA_api
    logging:  # <----------------- add the logging section
      driver: gelf
      options:
        gelf-address: "udp://localhost:5044"   # <------------ set the host name and port for logstash
    networks:
      - default
    environment:
      NODE_ENV: development
      PORT: ${API_PORT}
      FRONTEND_BASE_URL: ${FRONTEND_BASE_URL}
      SEND_GRID_API_KEY: ${SEND_GRID_API_KEY}
      SECRET_KEY: ${SECRET_KEY}
      MONGO_DB_CONNECTION_URL: ${MONGO_DB_CONNECTION_URL}
    ports:
      - "6753:6753"
```

We are sending the log as UDP packets in the `gelf` format. [gelf](https://go2docs.graylog.org/5-0/getting_in_log_data/ingest_gelf.html) stands for `Graylog Extended Log Format`

3. After running the application using either docker-compose or docker swarm. Go to `devOps/ELK/` directory in terminal and run:
```bash
docker compose up
```

4. Goto your browser http://0.0.0.0:5601 to access Kibana. Kibana is automatically linked with Elastic search.

5. After triggering events from `api`, or `socket` service you can see the logs in kibana. Logging in or hitting the api to get the contact list will trigger the logs.

6. Login to kibana using the following user name and password:
```
name: elastic
password: changeme
```
you can change the password through the .env file

7. After logging into Kibana, click the burger menu icon on the top left of the page, select stack management from the bottom of the list
![Kibana](./kibana1.png "Kibana1")

8. Then click on `Index Management` under `data section`
![Kibana](./kibana2.png "Kibana2")

9. If there are any logs collected, the index will be visible here. As of now the index are created on a daily basis. One index per day
![Kibana](./kibana3.png "Kibana3")

10. Now to see the actual logs, click on discover
![Kibana](./kibana4.png "Kibana4")

11. You need to create a data view. The indexes that you saw in the `Index Management` screen can be picked here. Just type the index name in the `Index pattern` text, in this case it can be something like `logstash-2024.04.08` accourding to the screenshot. Give a name and click on `save data view in kibana`
![Kibana](./kibana5.png "Kibana5")

12. If you select the correct time range you should be see the logs as shown below:
![Kibana](./kibana6.png "Kibana6")

# How it works:

1. We have setup [winston](https://www.npmjs.com/package/winston) logger and [@elastic/ecs-winston-format](https://www.npmjs.com/package/@elastic/ecs-winston-format) to create the logs for the NodeJS applications.

2. The Docker Daemon will record all the logs that are generated by the services (These are logs from winston as well as `console.log` in the code and other logs), you can refer to that in this [link](https://docs.docker.com/config/containers/logging/configure/). We can configure these logs at a service level also as show above. When the application generates the logs docker will triggered UDP packets to the configured host and port.

3. In the logstash configuration we have configured the input, output and filter sections.

```conf
input {
  gelf {
    id => "my_plugin_id"
    use_udp => true
    host => "0.0.0.0"
    port_udp => 5044
    port => 5044
  }
}

# it was throwing an error for the host value 
# being sent. Hence removing it.
filter {
    mutate {
      remove_field => [ "host" ]
    }
}

output {
  elasticsearch {
    index => "logstash-%{+YYYY.MM.dd}"
    hosts => ["https://es01:9200"]
    user => "elastic"
    password => "changeme"
    ssl_enabled => true
    cacert => "/usr/share/logstash/certs/ca/ca.crt"
  }
  #stdout {}
}
```
We can have multiple logstash pipelines. Here in this logstash pipeline we are listining at port 5044 in UDP using the [gelf input plugin](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-gelf.html) for logstash

4. Logstash will remove the host name. I was getting an error regarding the format of the data. It was expecting an object, but it was receiving a string. It will send the data to elasticsearch. We can set the index name here.

### 5. How SSL works:

- After elasticsearch version 8+, SSL is enabled by default. I had many failed attempts trying to disable ssl.
- the `setup` service in `devOps/ELK/docker-compose.yml` file will use a utility called `bin/elasticsearch-certutil` to generate the ssl certificates from within the container and store it in a volume called `certs`
- This will then be mounted to all the other services (es01,es02,kibana, and logstash) in the `devOps/ELK/docker-compose.yml` file.
- The services will use these certificates to communicate with the master elasticsearch node `es01`.

6. This is the SSL certificate that logstash uses in its configuration to send data to elasticsearch. We provide the user name, password, and the host.

7. Elastic will store these logs and Kibana is used to visualize the logs.

8. What I have noticed is, only logs generated by winston are collected by logstash, console.log messages are not getting collected.
